# Рекомендации по созданию персонажей и анимации

Чтобы сохранить консистентность персонажа во всех сценах и анимации, важно зафиксировать не только само изображение, но и **параметры генерации**, а также подготовить данные для последующей работы в других инструментах. Вот что нужно сделать:

---

### **1. Сохраните критически важные параметры из MidJourney**
Если вы сгенерировали персонажа в MidJourney:
- **Seed** (число, например `--seed 1234`) — ключевой параметр для воспроизведения похожих результатов.  
  *Как получить:* Добавьте `--seed <число>` в промпт или найдите seed в метаданных изображения через `/describe`.  
- **Точный текстовый промпт** — каждое слово влияет на результат. Пример:  
  `"young wizard, blue robe, silver staff, scar on left cheek, cinematic lighting, anime style --seed 1234 --v 6"`.  
- **Версия модели** (например, `--v 6`) — разные версии MidJourney дают разную стилистику.  
- **Ссылку на исходное изображение** — через `/show` в Discord, чтобы генерировать вариации на его основе.

---

### **2. Перенесите персонажа в Stable Diffusion для полного контроля**
MidJourney не позволяет тонко управлять консистентностью, поэтому для анимации и будущих сцен лучше перейти в **Stable Diffusion** (+ ControlNet).  

#### Как это сделать:
1. **Создайте эталонное изображение** в MidJourney (персонаж в нейтральной позе, фронтальный ракурс).  
2. **Сгенерируйте текстовое описание** персонажа на основе промпта MidJourney.  
3. **Используйте Stable Diffusion** с плагинами:  
   - **LoRA/LyCORIS** — тренируйте персонажа на основе эталонного изображения ([гайд](https://civitai.com/models/9251?modelVersionId=366412)).  
   - **ControlNet** — для сохранения позы, формы, черт лица через:  
     - **OpenPose** (скелет),  
     - **Canny/Lineart** (контуры),  
     - **IP-Adapter** (передача стиля).  
4. **Фиксируйте Seed** в Stable Diffusion для воспроизводимости.

---

### **3. Подготовьте "эталонные" изображения для анимации**
Для анимации персонажа в разных сценах вам понадобятся:  
- **Несколько ракурсов**: фронтальный, профиль, 3/4.  
- **Базовые эмоции**: нейтральное лицо, улыбка, злость и т.д.  
- **Опорные позы**: стоя, сидя, в движении.  

**Как создать:**  
- В MidJourney: используйте `/describe` на исходном изображении, затем модифицируйте промпты, добавляя `side view`, `sitting`, `angry expression`.  
- В Stable Diffusion: применяйте ControlNet + промпты вроде `(same character), sitting on a chair, side view, [описание внешности]`.

---

### **4. Инструменты для анимации с консистентностью**
Когда эталоны готовы:  
1. **Генерация видео:**  
   - **Runway Gen-2** или **Pika Labs** — загрузите эталонное изображение и задайте движение через текстовый промпт.  
   - **Stable Video Diffusion** — используйте изображение + текстовое описание с упоминанием деталей персонажа.  
2. **Контроль консистентности:**  
   - В **Stable Diffusion** применяйте:  
     - **LoRA** для сохранения стиля персонажа,  
     - **ControlNet Tile** для сохранения цветов и текстур,  
     - **Regional Prompting** — чтобы разные части кадра не "плыли".  
3. **Ручная корректировка:**  
   - Если нейросеть искажает детали, дорисуйте их в **Photoshop** или **Krita**, затем снова прогоните через ControlNet.

---

### **5. Если вы хотите остаться в MidJourney**
Для коротких проектов можно попробовать:  
- Использовать **/describe** + **/imagine** с оригинальным изображением, добавляя `same character, same style` в промпт.  
- Экспериментировать с **–cref** (Character Reference) в MidJourney v6 ([гайд](https://docs.midjourney.com/docs/character-reference)).  
- Но будьте готовы к тому, что консистентность будет ограниченной — MidJourney не предназначен для строгого сохранения персонажей.

---

### **Что сохранить в отдельный файл**
Создайте текстовый файл с такими данными:  
```markdown
- **Prompt:** "young wizard, blue robe, silver staff..."  
- **Seed:** 1234 (MidJourney) / 456789 (Stable Diffusion)  
- **Model:** MJ v6 / SDXL 1.0  
- **ControlNet settings:** OpenPose, weight=1.0  
- **LoRA:** Name: Wizard_LoRA, weight=0.8  
- **Эталонные изображения:** ссылки на фронтальный вид, профиль, эмоции  
- **Цвета:** HEX-коды одежды (например, синий плащ — #2E5AAC).  
```

---

### **Пример рабочего процесса**
1. Сгенерируйте персонажа в MidJourney, сохраните seed и промпт.  
2. Переведите его в Stable Diffusion через Img2Img + ControlNet.  
3. Создайте LoRA-модель персонажа на основе эталонов.  
4. Для каждой новой сцены в Stable Diffusion:  
   - Активируйте LoRA,  
   - Укажите seed,  
   - Задайте позу через ControlNet OpenPose.  
5. Анимируйте результат в Pika Labs, загружая изображения с фиксированными параметрами.

---

### **Советы**
- Для лиц используйте **текстурные инверсии** (Textual Inversion) в SD — это сохранит мимику.  
- Если одежда "плывет", задавайте ее явно в промпте: `wearing (exact same blue robe:1.3)`.  
- Для сложных проектов тренируйте **Dreambooth**-модель персонажа (но это требует GPU).  

Консистентность на 100% пока недостижима, но комбинация Seed + LoRA + ControlNet + ручной правки даст приемлемый результат.

---

Проект распространяется под лицензией [Creative Commons Attribution-NonCommercial-ShareAlike 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ru). Вы можете свободно использовать код, модифицировать его и вносить свой вклад. Просим сохранять упоминание авторов и соблюдать условия лицензии.